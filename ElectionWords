# Load all the packages we'll need
install.packages("devtools")
library(devtools)
install_github("govdat","cocteau")
library(govdat)
library(XML)
library(rjson)

# To test to see if everything loaded correctly, the help window should come up
?sll_cw_phrases

# Look into Wisconsin senators Tammy Baldwin and Ron Johnson
tammy <- sll_cw_phrases(entity_type="legislator",entity_value="B001230",key="ed33f4ec86894758b4b622f273b52240")
ron <- sll_cw_phrases(entity_type="legislator",entity_value="J000293",key="ed33f4ec86894758b4b622f273b52240")

# This gives us all of their words since they started recording them, 
# Making a loop to pull Tammy Baldwin's documents
transcript = data.frame()
p=0
newdata = sll_cw_text(bioguide_id="B001230",key="ed33f4ec86894758b4b622f273b52240",page=p)
while(!is.null(newdata)){
transcript=rbind(transcript,newdata)
p=p+1
newdata = sll_cw_text(bioguide_id="B001230",key="ed33f4ec86894758b4b622f273b52240",page=p)
}
      
# Making a loop to pull Ron Johnson's documents
transcript2 = data.frame()
p=0
newdata = sll_cw_text(bioguide_id="J000293",key="ed33f4ec86894758b4b622f273b52240",page=p)
while(!is.null(newdata)){
  transcript2=rbind(transcript2,newdata)
  p=p+1
  newdata = sll_cw_text(bioguide_id="J000293",key="ed33f4ec86894758b4b622f273b52240",page=p)
}

# We'll have to select the speech section from the date ranges we want before we can 
# work with it

# To select out Tammy's speeches for the year leading up to the election:
grep("2011",transcript$id)
grep("2012",transcript$id)
# These arn't concurrent like Ron's are, so we'll have to join them piece by peice
tt1=transcript[1:3,]
tt2=transcript[45:57,]
tt3=transcript[563:564,]
tt4=transcript[595:612,]
transcripttammy=rbind(tt1,tt2,tt3,tt4)
  
# To select out Ron's speeches for the year leading up to the election:
grep("2011",transcript2$id)
grep("2012",transcript2$id)
# We can then eyeball the rows to see which speeches happen between Nov. 6, 2011-Nov.5, 2012
transcriptron=transcript2[26:75,]

# Now we have to find which words are the most common in the year leading up to the election
# for Tammy
text = "Mr. Speaker, I move that the House do now adjourn."
matches=regexpr("\\b\\w+\\b", text)
regmatches(text,matches)
matches=gregexpr("\\b\\w+\\b", transcripttammy$speaking)
allwords = regmatches(transcripttammy$speaking,matches)
allwords[[1]]
allwords[[2]]
unlist(allwords)
matches=gregexpr("\\b\\w+\\b", transcripttammy$speaking)
allwords = tolower(unlist(regmatches(transcripttammy$speaking,matches)))
matches=gregexpr("\\b\\w+\\b", transcripttammy$speaking)
allwords = tolower(unlist(regmatches(transcripttammy$speaking,matches)))
sort(table(allwords))/length(allwords)

worddf = data.frame(word=names(allwords),count=allwords)
taballwords = table(allwords)
worddf = data.frame(word=names(taballwords),count=taballwords)
worddf = worddf(taballwords)
worddf = data.frame(taballwords)
words = data.frame(table(allwords))
words$Freq = words$Freq/sum(words$Freq)
order(words$Freq)
words[order(words$Freq),]
wordz=words[order(words$Freq,decreasing=TRUE),]
wordz$ranking=1:nrow(wordz)
View(wordz)

# Now we have to find which words2 are the most common in the year leading up to the election
text = "Mr. Speaker, I move that the House do now adjourn."
matches=regexpr("\\b\\w+\\b", text)
regmatches(text,matches)
matches=gregexpr("\\b\\w+\\b", transcriptron$speaking)
allwords22 = regmatches(transcriptron$speaking,matches)
allwords22[[1]]
allwords22[[2]]
unlist(allwords22)
matches=gregexpr("\\b\\w+\\b", transcriptron$speaking)
allwords22 = tolower(unlist(regmatches(transcriptron$speaking,matches)))
matches=gregexpr("\\b\\w+\\b", transcriptron$speaking)
allwords22 = tolower(unlist(regmatches(transcriptron$speaking,matches)))
sort(table(allwords22))/length(allwords22)

worddf2 = data.frame(word=names(allwords22),count=allwords22)
taballwords22 = table(allwords22)
worddf2 = data.frame(word=names(taballwords22),count=taballwords22)
worddf2 = worddf2(taballwords22)
worddf2 = data.frame(taballwords22)
words2 = data.frame(table(allwords22))
words2$Freq = words2$Freq/sum(words2$Freq)
order(words2$Freq)
words2[order(words2$Freq),]
wordz2=words2[order(words2$Freq,decreasing=TRUE),]
wordz2$ranking=1:nrow(wordz2)
View(wordz2)

# Now that we've got our two tables, we need to merge them, subtracting the difference
# between the chart positions so we can see which words they don't say 
wordz2$allwords=wordz2$allwords22
wordmerge=merge(wordz,wordz2,by="allwords")
wordmerge$diff=sqrt((wordmerge$ranking.x - wordmerge$ranking.y)^2)
newmerge=wordmerge[order(wordmerge$diff,decreasing=TRUE),]
# That's a lot of differentiation between the words, we'll reduce it to the top 1000 words
wordza=wordz[1:1500,]
wordz2a=wordz2[1:1500,]
wordmerge2=merge(wordza,wordz2a,by="allwords")
wordmerge2$diff=sqrt((wordmerge2$ranking.x - wordmerge2$ranking.y)^2)
newmerge2=wordmerge2[order(wordmerge2$diff,decreasing=TRUE),]
View(newmerge2)


                         

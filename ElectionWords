# Load all the packages we'll need
install.packages("devtools")
library(devtools)
install_github("govdat","cocteau")
library(govdat)
library(XML)
library(rjson)
?sll_cw_phrases

# Look into Wisconsin senators Tammy Baldwin and Ron Johnson
tammy<-sll_cw_phrases(entity_type="legislator",entity_value="B001230",n=1,sort="100",key="434825d810f6409aa6586bd150d6978a")
ron<-sll_cw_phrases(entity_type="legislator",entity_value="J000293",n=1,sort="100",key="434825d810f6409aa6586bd150d6978a")

# Making a loop to pull Tammy's documents
transcript = data.frame()
p=0
newdata = sll_cw_text(bioguide_id="B001230",key="ed33f4ec86894758b4b622f273b52240",page=p)
while(!is.null(newdata)){
transcript=rbind(transcript,newdata)
p=p+1
newdata = sll_cw_text(bioguide_id="B001230",key="ed33f4ec86894758b4b622f273b52240",page=p)
}

# Making a loop to pull Ron's documents
transcript2 = data.frame()
p=0
newdata = sll_cw_text(bioguide_id="J000293",key="ed33f4ec86894758b4b622f273b52240",page=p)
while(!is.null(newdata)){
  transcript2=rbind(transcript2,newdata)
  p=p+1
  newdata = sll_cw_text(bioguide_id="J000293",key="ed33f4ec86894758b4b622f273b52240",page=p)
}

# Now we have to find which words are the most common in the year leading up to the election


sort(table(allwords))/length(allwords)

worddf = data.frame(word=names(allwords),count=allwords)
taballwords = table(allwords)
worddf = data.frame(word=names(taballwords),count=taballwords)
worddf = worddf(taballwords)
worddf = data.frame(taballwords)
words = data.frame(table(allwords))
words$Freq = words$Freq/sum(words$Freq)
order(words$Freq)
words[order(words$Freq),]
wordz=words[order(words$Freq,decreasing=TRUE),]
wordz$ranking=1:nrow(wordz)
View(wordz)

